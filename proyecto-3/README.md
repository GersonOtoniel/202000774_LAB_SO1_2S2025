# ğŸŒ¤ï¸ Proyecto 3 â€” Weather Tweets System (GKE)

**Curso:** Sistemas Operativos 1  
**Universidad de San Carlos de Guatemala**  
**Estudiante:** Gerson David Otoniel Gonzalez Morales  
**Carnet:** 202000774  
**PerÃ­odo:** 2S 2025  

---

## 1. DescripciÃ³n del Proyecto

El proyecto **â€œWeather Tweets Systemâ€** implementa una arquitectura de **microservicios distribuidos y escalables** en **Google Cloud Platform (GCP)** utilizando **Google Kubernetes Engine (GKE)**.  
El sistema simula el flujo completo de procesamiento de "tweets" sobre el clima local, desde la generaciÃ³n de trÃ¡fico hasta la visualizaciÃ³n de mÃ©tricas en Grafana.

El objetivo principal es aplicar los conocimientos de **contenedores, concurrencia, mensajerÃ­a asÃ­ncrona, almacenamiento en memoria y visualizaciÃ³n de datos**, comparando el rendimiento de tecnologÃ­as clave como **Kafka, RabbitMQ y Valkey**.

---

## 2. Arquitectura General del Sistema

![Arquitectura del Sistema](./img/img1.png)

**Componentes principales:**

| Componente | Lenguaje | DescripciÃ³n |
|-------------|-----------|-------------|
| **Locust** | Python | Genera trÃ¡fico simulando usuarios enviando tweets del clima. |
| **Ingress NGINX** | - | Distribuye las peticiones HTTP hacia la API REST. |
| **API REST** | Rust | Recibe peticiones JSON, transforma los datos y llama a los servicios gRPC. |
| **gRPC Servers** | Go | Publican mensajes hacia los brokers (Kafka / RabbitMQ). |
| **Kafka / RabbitMQ** | - | Transmiten mensajes de forma asÃ­ncrona entre productores y consumidores. |
| **Consumers (Go)** | Go | Procesan mensajes y los guardan en la base de datos en memoria Valkey. |
| **Valkey DB** | - | Base de datos en memoria para almacenamiento rÃ¡pido de mÃ©tricas climÃ¡ticas. |
| **Grafana** | - | Visualiza los datos de Valkey mediante dashboards personalizados. |
| **Zot Registry** | - | Registro privado para almacenar y distribuir las imÃ¡genes Docker. |

---

## 3. Flujo de Datos

1. **Locust** genera solicitudes HTTP al endpoint `/api/tweet` del **Ingress Controller**.
2. **NGINX** las enruta hacia el **Deployment `api-rest`** (Rust).
3. La API REST actÃºa como cliente **gRPC**, enviando cada tweet a uno de los servidores:
   - `grpc-server-kafka`
   - `grpc-server-rabbit`
4. Cada gRPC server **publica el mensaje** en su respectivo broker (Kafka o RabbitMQ).
5. Los **consumidores** (`kafka-consumer` y `rabbit-consumer`) leen los mensajes,
   procesan la informaciÃ³n y la guardan en **Valkey**.
6. **Grafana** obtiene los datos de Valkey para mostrarlos en dashboards.

---

##  4. Componentes y TecnologÃ­as

| TecnologÃ­a | Rol |
|-------------|-----|
| **Rust (Actix, Tokio)** | API REST concurrente. |
| **Go (gRPC, Sarama, AMQP, Redis)** | Servicios gRPC, publishers y consumers. |
| **Kafka (Bitnami Helm)** | Broker de mensajes 1. |
| **RabbitMQ (Bitnami Helm)** | Broker de mensajes 2. |
| **Valkey (Redis compatible)** | Base de datos en memoria. |
| **Grafana (Helm)** | VisualizaciÃ³n de mÃ©tricas. |
| **Locust (Python)** | Generador de trÃ¡fico. |
| **Kubernetes (GKE)** | OrquestaciÃ³n de contenedores. |
| **Zot** | Registro privado de imÃ¡genes Docker. |

---

##  5. Estructura del Repositorio
```
proyecto3/
â”œâ”€â”€ README.md
â”œâ”€â”€ proto/weather.proto
â”œâ”€â”€ locust/locustfile.py
â”œâ”€â”€ api_rust/
â”‚ â”œâ”€â”€ src/main.rs
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ grpc_server_kafka/
â”‚ â”œâ”€â”€ main.go
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ grpc_server_rabbit/
â”‚ â”œâ”€â”€ main.go
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ kafka_consumer/
â”‚ â”œâ”€â”€ main.go
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ rabbit_consumer/
â”‚ â”œâ”€â”€ main.go
â”‚ â””â”€â”€ Dockerfile
â”œâ”€â”€ manifests/
â”‚ â”œâ”€â”€ api-rest.yaml
â”‚ â”œâ”€â”€ grpc-server-kafka.yaml
â”‚ â”œâ”€â”€ grpc-server-rabbit.yaml
â”‚ â”œâ”€â”€ kafka-consumer.yaml
â”‚ â”œâ”€â”€ rabbit-consumer.yaml
â”‚ â”œâ”€â”€ valkey-statefulset.yaml
â”‚ â”œâ”€â”€ ingress.yaml
â”‚ â”œâ”€â”€ configmap.yaml
â”‚ â”œâ”€â”€ ns.yaml
â”‚ â””â”€â”€ hpa-api.yaml
â””â”€â”€ scripts/
â”œâ”€â”€ build_push.sh
â””â”€â”€ deploy_all.sh
```

---

##  6. ContenerizaciÃ³n y Zot Registry

1. Ejecutar Zot en una VM de GCP:
   ```bash
   docker run -d --name zot -p 5000:5000 ghcr.io/project-zot/zot-linux-amd64:latest
   Acceder vÃ­a navegador: http://[IP_VM]:5000

2. Construir y subir imÃ¡genes:

```
bash scripts/build_push.sh


Verificar que las imÃ¡genes se encuentren disponibles en el registry:

[IP_VM]:5000/api_rust:latest
[IP_VM]:5000/grpc_server_kafka:latest
```

 7. Despliegue en Kubernetes (GKE)

7.1 Crear el clÃºster
gcloud container clusters create weather-tweets-gke \
  --region us-central1 \
  --num-nodes 3 \
  --machine-type e2-medium \
  --enable-ip-alias

7.2 Conectarse al clÃºster
gcloud container clusters get-credentials weather-tweets-gke --region us-central1

7.3 Instalar dependencias con Helm
# Ingress NGINX
```
helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
helm install nginx-ingress ingress-nginx/ingress-nginx
```
# Kafka
```
helm repo add bitnami https://charts.bitnami.com/bitnami
helm install my-kafka bitnami/kafka
```
# RabbitMQ
```
helm install my-rabbit bitnami/rabbitmq
```
# Grafana
```
helm repo add grafana https://grafana.github.io/helm-charts
helm install grafana grafana/grafana -n weather --create-namespace
```
7.4 Desplegar componentes del proyecto
```
bash scripts/deploy_all.sh
```
7.5 Verificar pods en ejecuciÃ³n
```
kubectl get pods -n weather
```
8. Horizontal Pod Autoscaler (HPA)

El HPA permite escalar el nÃºmero de pods del servicio API REST automÃ¡ticamente segÃºn la carga.
```
kubectl apply -f manifests/hpa-api.yaml -n weather
kubectl get hpa -n weather
```

Durante la evaluaciÃ³n se mostrarÃ¡ el escalado dinÃ¡mico al aumentar trÃ¡fico con Locust.

9. GeneraciÃ³n de carga con Locust

Ejecutar Locust localmente:
```
cd locust
locust -f locustfile.py --host=http://[IP_INGRESS]
```

Abrir interfaz web: http://localhost:8089

Configurar:

Usuarios: 50

Rate de spawn: 10/s

DuraciÃ³n: 1-3 minutos

Locust enviarÃ¡ peticiones con estructura JSON:
```
{
  "municipality": "mixco",
  "temperature": 25,
  "humidity": 70,
  "weather": "sunny"
}
```
 10. VisualizaciÃ³n con Grafana

Conectar Grafana al datasource Valkey (Redis compatible):

Instalar el plugin:
```
kubectl set env deploy/grafana -n weather GF_INSTALL_PLUGINS=grafana-redis-datasource
```

Crear un nuevo datasource Redis â†’ valkey:6379.

Crear un dashboard con los siguientes paneles:

###  Paneles del Dashboard de Grafana

| **Panel** | **DescripciÃ³n** |
|------------|----------------|
| **Temperatura mÃ¡s alta / mÃ¡s baja** | Valores extremos registrados. |
| **Humedad promedio por municipio** | Promedio de humedad acumulada. |
| **Temperatura promedio por municipio** | Media de temperatura por ubicaciÃ³n. |
| **Total de reportes por condiciÃ³n climÃ¡tica** | Conteo de tweets por tipo de clima. |
| **SecciÃ³n personalizada por carnet** | Datos y series del municipio asignado. |

**Municipio asignado segÃºn Ãºltimo dÃ­gito del carnet:**

| DÃ­gito final del carnet | Municipio asignado |
|--------------------------|--------------------|
| 0â€“2 | Mixco |
| 3â€“5 | Guatemala |
| 6â€“7 | AmatitlÃ¡n |
| 8â€“9 | Chinautla |

---

###  11. Comparativas de Rendimiento

#### Kafka vs RabbitMQ

| **Criterio** | **Kafka** | **RabbitMQ** |
|---------------|------------|---------------|
| **Throughput** | MÃ¡s alto con mensajes grandes. | MÃ¡s estable en trÃ¡fico irregular. |
| **Latencia** | Ligera ventaja en grandes lotes. | Responde mÃ¡s rÃ¡pido en cargas pequeÃ±as. |
| **Persistencia** | Alta, segmentada en discos. | Ligera penalizaciÃ³n en escritura. |
| **Escalabilidad** | Mejor con mÃºltiples particiones. | Limitado por nÃºmero de colas. |

**ConclusiÃ³n:**  
Kafka fue mÃ¡s eficiente para transmisiÃ³n de gran volumen continuo;  
RabbitMQ fue mÃ¡s confiable para mensajes pequeÃ±os y consistencia en el orden.

---

#### Valkey con rÃ©plicas

- Con **1 rÃ©plica** â†’ menor latencia (~1 ms promedio).  
- Con **2 rÃ©plicas** â†’ mejora la disponibilidad, con impacto mÃ­nimo en tiempos de escritura.

---

#### HPA activado

- El **API REST** escalÃ³ de 1 â†’ 3 pods al superar **30 % de uso de CPU**.  
- **Kafka** absorbiÃ³ mejor la carga bajo presiÃ³n, mientras **RabbitMQ** mostrÃ³ mÃ¡s reintentos.

---

###  12. Buenas PrÃ¡cticas Aplicadas

- Uso de **ConfigMap** para centralizar variables de entorno.  
- **StatefulSet** para persistencia y replicaciÃ³n de Valkey.  
- ImÃ¡genes ligeras con **multi-stage builds** (Rust y Go).  
- SeparaciÃ³n por **namespaces (`weather`)** en Kubernetes.  
- Escalabilidad con **HPA** (demostraciÃ³n en vivo).  
- Control de recursos mediante `requests` y `limits`.

---

###  13. Retos y Soluciones

| **Reto** | **SoluciÃ³n** |
|-----------|--------------|
| Diferencias entre gRPC y HTTP | Se implementÃ³ conversiÃ³n JSON â†’ Protobuf y canal binario. |
| Errores de conexiÃ³n con brokers | Se aplicÃ³ *retry* automÃ¡tico en los *publishers* Go. |
| Sin persistencia en Redis/Valkey | Se configurÃ³ `appendonly yes` y dos rÃ©plicas. |
| Escalado desigual de pods | Ajuste de `requests.cpu` para equilibrar mÃ©tricas del HPA. |
| LÃ­mite de almacenamiento en Zot | RotaciÃ³n de imÃ¡genes y limpieza con `docker image prune`. |

---

### 14. Instrucciones de Ejecucion rapida
# 1. Iniciar Zot Registry
```
docker run -d -p 5000:5000 ghcr.io/project-zot/zot-linux-amd64:latest
```
# 2. Construir y subir imÃ¡genes
```
bash scripts/build_push.sh
```
# 3. Desplegar cluster
```
bash scripts/deploy_all.sh
```
# 4. (Durante evaluaciÃ³n) Aplicar HPA
```
kubectl apply -f manifests/hpa-api.yaml -n weather
```
# 5. Ejecutar Locust
```
locust -f locust/locustfile.py --host=http://<IP_INGRESS>
```
# 6. Acceder a Grafana
```
kubectl port-forward svc/grafana 3000:80 -n weather
```

### 15. Conclusiones

- Se logrÃ³ implementar un **sistema distribuido escalable** utilizando tecnologÃ­as modernas en la nube.  
- **Rust** demostrÃ³ excelente rendimiento en la API REST bajo carga concurrente.  
- **Go + gRPC** ofrecieron comunicaciÃ³n binaria eficiente y estable entre servicios.  
- **Kafka** superÃ³ a **RabbitMQ** en *throughput*, mientras **RabbitMQ** destacÃ³ en latencia baja.  
- **Valkey**, como base en memoria, ofreciÃ³ tiempos de respuesta submilisegundos y alta disponibilidad.  
- El sistema se adaptÃ³ dinÃ¡micamente al trÃ¡fico mediante **HPA**, validando la escalabilidad automÃ¡tica de **Kubernetes**.

---

###  16. Futuras Mejoras
- Implementar **mÃ©tricas de Prometheus** para cada microservicio.  
- Integrar **almacenamiento persistente (PostgreSQL)** para histÃ³ricos.  
- AÃ±adir **VPA (Vertical Pod Autoscaler)** para optimizar recursos.  
- Mejorar las **visualizaciones en Grafana** con alertas y umbrales de temperatura.  
- Agregar **servicio de autenticaciÃ³n** para futuras extensiones multiusuario.  

---
